<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shen Zheng</title>
  
  <meta name="author" content="Shen Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<head>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>

<body>

  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shen Zheng „ÄåÈÉëÊ∑±„Äç</name>
              </p>


            <p>
            I am a second-year PhD student at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, Carnegie Mellon University (CMU), 
            advised by <a href="http://www.cs.cmu.edu/~srinivas/">Dr. Srinivasa Narasimhan</a>. 
            I have interned at <a href="https://waymo.com/">Waymo</a> and <a href="https://www.momenta.cn/">Momenta</a>, 
            and worked at <a href="https://lucidmotors.com/">Lucid Motors</a>. 
            I completed my <a href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-computer-vision/">M.S. in Computer Vision (MSCV)</a> at CMU, 
            and earned my B.S. in Mathematics from <a href="https://www.wku.edu.cn/">Wenzhou-Kean University (WKU)</a>, 
            where I worked with <a href="https://wku.edu.cn/faculty/gaurav-gupta/">Dr. Gaurav Gupta</a>.
            </p>

              <p>
                Email1: shenzhen@andrew.cmu.edu
              </p>

              <p>
                Email2: lebronshenzheng@gmail.com
              </p>

              

              <p style="text-align:center">
                <a href="images/Self/CV_ShenZheng.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=QV13_T8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ShenZheng2000">Github</a> &nbsp/&nbsp
                <a href="https://leetcode.com/LebronZheng/">Leetcode</a> &nbsp/&nbsp
                <a href="https://www.youtube.com/channel/UCwsy6n3UFfK2EavTfK1bI5w/playlists">YouTube</a>
              </p>
              
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Self/front2.jpg"><img style="width:75%;max-width:75%" alt="profile photo" src="images/Self/front2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td id="research-areas" style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research Areas</heading>

                <p>
  My current research focuses on robust understanding of long-tail yet safety-critical scenarios in autonomous driving
  (e.g., low-light, bad weather, work zones) using the following strategies:
</p>

<li><strong>Data Expansion:</strong>
  Collecting and synthesizing rare scenes 
  (e.g., <i><a href="#roadwork">ROADWork</a></i>, <i><a href="#tpsence">TPSeNCE</a></i>) and enriching existing datasets 
  with saliency-aware transformations 
  (e.g., <i><a href="#instancewarp">Instance-Warp</a></i>).
</li>

<li><strong>Modality Expansion:</strong>
  Integrating vision and language, and incorporating short-term and long-term memory.
</li>

<p>
  My earlier works focused on image restoration and enhancement 
  (e.g., <i><a href="#sgz">SGZ</a></i>, <i><a href="#lliesurvey">LLIE_Survey</a></i>).
</p>


              </td>
            </tr>
          </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

          </tbody>
        </table>

 

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <p>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>




          <tr onmouseout="Road_Stop()" onmouseover="Road_start()" id="roadwork">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/RoadWork/roadwork.png" width="160"></div>
                <img src="images/RoadWork/roadwork.png" width="160">
              </div>
              <script type="text/javascript">
                function Road_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function Road_Stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                Road_Stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>ROADWork Dataset: Learning to Recognize, Observe, Analyze and Drive Through Work Zones</papertitle>
              <a href="#research-areas">
                <i class="fa-solid fa-house"></i>
              </a>
              <br>
              Anurag Ghosh, <strong>Shen Zheng</strong>, Robert Tamburo, Juan R. Alvarez Padilla, Hailiang Zhu, Michael Cardei, Nicholas Dunn, Christoph Mertz, Srinivasa Narasimhan
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">ICCV 2025 </strong> 
              </em>
              <br>
              <a href="https://arxiv.org/abs/2406.07661">[Paper]</a>
               <a href="http://www.cs.cmu.edu/~ILIM/roadwork_dataset/">[Webpage]</a>
               <a href="https://github.com/anuragxel/roadwork-dataset">[GitHub]</a>
              <br>
              <p>
                <strong>Motivation</strong>: 
                Navigating through work zones is challenging due to a lack of large-scale open datasets.
              </p>
              <p>
                <strong>Solution</strong>: 
                Introduce the ROADWork dataset, which is so far the largest open-source work zone dataset, to help learn how to recognize, observe, analyze, and drive through work zones.
              </p>
  
            </td>
          </tr>






          <tr onmouseout="Inst_stop()" onmouseover="Inst_start()" id="instancewarp">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/Inst_Warp/da-badweather.png" width="160"></div>
                <img src="images/Inst_Warp/da-badweather.png" width="160">
              </div>
              <script type="text/javascript">
                function Inst_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function Inst_stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                Inst_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Instance-Warp: Saliency Guided Image Warping for Unsupervised Domain Adaptation</papertitle>
              <a href="#research-areas">
                <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
              </a>
              <br>
              <strong>Shen Zheng‚òÖ</strong>,
                    Anurag Ghosh‚òÖ,
                    Srinivasa Narasimhan
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">WACV 2025 </strong> 
              </em>
              <br>
              <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Zheng_Instance-Warp_Saliency_Guided_Image_Warping_for_Unsupervised_Domain_Adaptation_WACV_2025_paper.pdf">[Paper]</a>
               <a href="https://instancewarp.github.io/">[Webpage]</a>
               <a href="https://github.com/ShenZheng2000/Instance-Warp">[Code]</a>
              <br>
              <p>
                <strong>Motivation</strong>: 
                Domain adaptation methods struggle to learn smaller objects amidst dominant backgrounds with high cross-domain variations.
              </p>
              <p>
                <strong>Solution</strong>: 
                Warp source-domain images in-place using instance-level saliency to oversample objects and undersample backgrounds during domain adaptation training.
              </p>
            </td>
          </tr>




          <tr onmouseout="TPSeNCE_stop()" onmouseover="TPSeNCE_start()" id="tpsence">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/TPSeNCE/Titlepage.png" width="160"></div>
                <img src='images/TPSeNCE/Titlepage.png' width="160">
              </div>
              <script type="text/javascript">
                function TPSeNCE_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function TPSeNCE_stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                TPSeNCE_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain</papertitle>
              <a href="#research-areas">
                <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
              </a>
              <br>
              <strong>Shen Zheng</strong>,
                    Changjie Lu,
                    Srinivasa Narasimhan
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">WACV 2024</strong> 
              </em>
              <br>
              <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Zheng_TPSeNCE_Towards_Artifact-Free_Realistic_Rain_Generation_for_Deraining_and_Object_WACV_2024_paper.pdf">[Paper]</a>
              <a href="https://shenzheng2000.github.io/tpsence.github.io/">[Webpage]</a>
              <a href="https://github.com/ShenZheng2000/TPSeNCE/">[Code]</a>
              <!-- <a href="https://openaccess.thecvf.com/content/WACV2024/supplemental/Zheng_TPSeNCE_Towards_Artifact-Free_WACV_2024_supplemental.pdf">Supp</a> | -->
              <a href="images/TPSeNCE/TPSeNCE_Slides.pdf">[Slides]</a>
              <a href="images/TPSeNCE/TPSeNCE_Poster.pdf">[Poster]</a>
              <p>
                <strong>Motivation</strong>: 
                Previous image-to-image translation methods produce artifacts and distortions, and lack control over the amount of rain generated. 
              </p>
              <p>
                <strong>Solution</strong>: 
                Introduce a Triangular Probability Similarity (TPS) loss to minimize the artifacts and distortions during rain generation.  
                Propose a Semantic Noise Contrastive Estimation (SeNCE) strategy to optimize the amounts of generated rain. 
                Show that realistic rain generation benefits deraining and object detection in rain. 
              </p>
  
            </td>
          </tr>
      



          <tr onmouseout="LLIE_Survey_stop()" onmouseover="LLIE_Survey_start()"  id="lliesurvey">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/LLIE_Survey/Timeline.png" width="160"></div>
                <img src='images/LLIE_Survey/Timeline.png' width="160">
              </div>
              <script type="text/javascript">
                function LLIE_Survey_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function LLIE_Survey_stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                LLIE_Survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2207.06324"> -->
                <papertitle>Low-Light Image Enhancement: A Comprehensive Survey and Beyond</papertitle>
              <!-- </a> -->
              <a href="#research-areas">
                <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
              </a>
              <br>
              <strong>Shen Zheng</strong>,
              Yiling Ma,
              Jinqian Pan,
              Changjie Lu,
              Gaurav Gupta
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;"></strong> 
              </em>
              <br>
              <a href="https://arxiv.org/abs/2212.10772">[Paper]</a>
              <a href="https://github.com/ShenZheng2000/LLIE_Survey">[Code]</a>
              <p>
                <strong>Motivation</strong>: 
                Existing LLIE datasets focus on either overexposure or underexposure, not both, and usually feature minimally degraded images captured from static positions.
              </p>
              <p>
                <strong>Solution</strong>: 
                Present a comprehensive survey of low-light image enhancement (LLIE).
                Propose the SICE_Grad and SICE_Mix image datasets, which include images with both overexposure and underexposure. 
                Introduce Night Wenzhou, a large-scale, high-resolution video dataset captured in fast motion with diverse illuminations and degradation.
              </p>
  
            </td>
          </tr>
  



    <tr onmouseout="Point_cloud_stop()" onmouseover="Point_cloud_start()" id="pointnorm">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='wacv'>
            <img src="images/PointNorm/PointNorm.png" width="160">
          </div>
          <img src='images/PointNorm/PointNorm.png' width="160">
        </div>
        <script type="text/javascript">
          function Point_cloud_start() {
            document.getElementById('wacv').style.opacity = "1";
          }

          function Point_cloud_stop() {
            document.getElementById('wacv').style.opacity = "0";
          }
          Point_cloud_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <papertitle>PointNorm: Dual Normalization is All You Need for Point Cloud Analysis</papertitle>
        <a href="#research-areas">
          <i class="fa-solid fa-house"></i>
        </a>
        <br>
        <strong>Shen Zheng</strong>,
        Jinqian Pan,
        Changjie Lu,
        Gaurav Gupta
        <br>
        <em style="display: block; margin: 10px 0 -5px;"> 
          <strong style="color: red;">IJCNN 2023 (Oral Presentation)</strong> 
        </em>
        <br>
        <a href="https://arxiv.org/abs/2207.06324">[Paper]</a>
        <a href="https://shenzheng2000.github.io/pointnorm.github.io/">[Webpage]</a>
        <a href="https://github.com/ShenZheng2000/PointNorm-for-Point-Cloud-Analysis">[Code]</a>
        <a href="images/PointNorm/IJCNN_2023_Pre.pdf">[Slides]</a> 
        <p>
          <strong>Motivation</strong>: Current point cloud analysis methods struggle with irregular (i.e., unevenly distributed) point clouds. 
        </p>
        <p>
          <strong>Solution</strong>: PointNorm, a point cloud analysis network with a DualNorm module (Point Normalization & Reverse Point Normalization) that leverages local mean and global standard deviation.
        </p>
      </td>
    </tr>



        <tr onmouseout="LLIE_stop()" onmouseover="LLIE_start()" id="sgz">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='night'>
              <img src="images/SGZ/F1.png" width="160"></div>
              <img src='images/SGZ/F1Crop.png' width="160">
            </div>
            <script type="text/javascript">
              function LLIE_start() {
                document.getElementById('night').style.opacity = "0";
              }

              function LLIE_stop() {
                document.getElementById('night').style.opacity = "1";
              }
              LLIE_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <a href="images/SGZ/SGZ.pdf"> -->
              <papertitle>Semantic-Guided Zero-Shot Learning for Low-Light Image/Video Enhancement</papertitle>
            <!-- </a> -->
            <a href="#research-areas">
              <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
            </a>
            <br>
            <strong>Shen Zheng</strong>, 
            Gaurav Gupta
            <br>
            <em style="display: block; margin: 10px 0 -5px;"> 
              <strong style="color: red;">WACV 2022</strong> 
            </em>
            <br>
            <a href="https://arxiv.org/pdf/2110.00970.pdf">[Paper]</a>
            <a href="https://shenzheng2000.github.io/sgz.github.io/">[Webpage]</a>
            <a href="https://github.com/ShenZheng2000/Semantic-Guided-Low-Light-Image-Enhancement">[Code]</a>
            <a href="images/SGZ/SGZ_Slides.pdf">[Slides]</a>
            <p>
              <strong>Motivation</strong>: Current low-light image enhancement methods cannot handle uneven illuminations, is computationally inefficient, and fail to preserve the semantic information. 
            </p>
            <p>
              <strong>Solution</strong>: Introduce SGZ, a zero-shot low-light image enhancement framework with pixel-wise light deficiency estimation, parameter-free recurrent image enhancement, and unsupervised semantic segmentation.
            </p>
          </td>
        </tr>


        
        <tr onmouseout="ACML_stop()" onmouseover="ACML_start()"  id="asintrovae">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='acml'>
              <img src="images/AS_IntroVAE/AS_IntroVAE.JPG" width="160"></div>
              <img src='images/AS_IntroVAE/AS_IntroVAE.JPG' width="160">
            </div>
            <script type="text/javascript">
              function ACML_start() {
                document.getElementById('acml').style.opacity = "1";
              }

              function ACML_stop() {
                document.getElementById('acml').style.opacity = "0";
              }
              UDA_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <a href="images/AS_IntroVAE/AS_IntroVAE_Paper.pdf"> -->
              <papertitle>AS-IntroVAE: Adversarial Similarity Distance Makes Robust IntroVAE</papertitle>
            <!-- </a> -->
            <a href="#research-areas">
              <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
            </a>
            <br>
            Changjie Lu, 
            <strong>Shen Zheng</strong>, 
            Zirui Wang,
            Omar Dib,
            Gaurav Gupta
            <br>
            <em style="display: block; margin: 10px 0 -5px;"> 
              <strong style="color: red;">ACML 2022</strong> 
            </em> 
            <br>
            <!-- <a href="">code</a> / -->
            <a href="https://arxiv.org/pdf/2206.13903.pdf">[Paper]</a>       
            <a href="https://github.com/ShenZheng2000/SAPNet-for-image-deraining">[Code]</a>
            <a href="images/AS_IntroVAE/AS_IntroVAE_Slides.pdf">[Slides]</a>
            <p>
              <strong>Motivation</strong>: Generative models experience posterior collapse and vanishing gradient due to no effective metric for real-fake image evaluation.
            </p>
            <p>
              <strong>Solution</strong>: Propose Adversarial Similarity Distance Introspective Variational Autoencoder (AS-IntroVAE), which can address the posterior
              collapse and the vanishing gradient problem in image generation in one go. 
            </p>
          </td>
        </tr>







        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Experiences</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Waymo/Waymo_logo.svg.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong>Perception Software Engineer (Intern)</strong> at <a href="https://waymo.com/">Waymo</a>

              <p>
                Mentor: <a href="https://www.linkedin.com/in/kylekh">Guohao Zhang</a>
              </p>

              <p>
               (WIP) Improved Online HD Map Construction using long-term and short-term memory fusion. 
            </p>



            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Lucid/images.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong>Perception Software Engineer (Full-Time)</strong> at <a href="https://lucidmotors.com/">Lucid Motors</a>

              <p>
                Director: <a href="https://www.linkedin.com/in/feng-guo-7126594/">Dr. Feng Guo</a>
              </p>

              <p>
                Working as a perception software engineer in the ADAS perception team responsible for auto-parking, traffic light detection, and blockage detection.
            </p>

            <p>
              Improved BEVFormer for auto-parking (reverse & parallel) by using extrinsic calibration to interpolate and smooth edges to enhance curb detection.
            </p>

            <p>
              Trained YOLO6 on full-resolution images containing traffic lights and fine-tuned arrow types, confidence, IoU, and area thresholds, resulting in a 40+% improvement in mAP (final mAP: 98%+ for day; 90%+ for night).
            </p>

            <p>
              Developed a binary semantic segmentation model based on CenterNet to detect blockages such as ice, snow, mud, mud blur, rain drops, and sun glares, achieving 93%+ IoU. 
            </p>

            <!-- <p>
              Improved battery template matching to ensure success across all captured battery images, including in low-light or overexposed conditions.
            </p> -->


            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Momenta/Momenta.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong>Perception Engineer (Intern) </strong> at <a href="https://www.momenta.cn/">Momenta</a>

              <p>
                Director: <a href="https://scholar.google.com.sg/citations?user=Wo5Cem4AAAAJ&hl=en">Dr. Wangjiang Zhu</a>
                 
              </p>

              <p>
              Responsible for long-tailed data augmentation, training data auto-labeling and cleaning, and model evaluation for traffic light detection algorithms.
            </p>

            <p>
              Implemented CycleGAN to conduct unsupervised data augmentation, converting traffic light bulbs from left arrow to round & leftUturn arrow.
            </p>

            <p>
              Constructed a traffic light auto-label model using quantized VoVNet-57, filtering 14,618 incorrect annotations from 1,160,513 labeled frames.
            </p>

            <p>
              Increased the classification accuracy for leftUturn traffic light from 78.41% to 87.27%, and the mean average precision from 93.01% to 94.80%.
            </p>
            </td>
          </tr>
 
    


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Services/CVF.jpg" style="width: 185px;"></td>
            <td width="75%" valign="center">

              <strong>Technical Program Committee</strong>: 
              <br>
              <a href="https://2024.ieeewcci.org/">WCCI 2024</a>
              <br>
              <br>

              <strong>Conference Reviewers</strong>: 
              <br>
              CVIP (<a href="https://sites.google.com/view/cvip-2021/home">2021</a>, <a href="https://vnit.ac.in/cvip2022/">2022</a>), 
              AAAI (<a href="https://aaai.org/Conferences/AAAI-22/">2022</a>), 
              IJCNN (<a href="https://2023.ijcnn.org/">2023</a>, <a href="https://2024.ijcnn.org/">2024</a>, <a href="https://2025.ijcnn.org/">2025</a>), 
              WACV (<a href="https://wacv2023.thecvf.com/home">2023</a>, <a href="https://wacv2024.thecvf.com/home">2024</a>, <a href="https://wacv2025.thecvf.com/home">2025</a>), 
              ECCV (<a href="https://eccv2024.ecva.net/">2024</a>), 
              CVPR (<a href="https://cvpr.thecvf.com/">2025,2026</a>), 
              ICCV (<a href="https://iccv.thecvf.com/">2025</a>)
              <br>
              <br>

              <strong>Journal Reviewers</strong>:
              <br>
              <a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems">TNNLS</a>,
              <a href="https://www.springer.com/journal/11263">IJCV</a>,
              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">TCSVT</a>,
              <a href="https://www.sciencedirect.com/journal/expert-systems-with-applications">ESWA</a>,
              <a href="https://www.sciencedirect.com/journal/engineering-applications-of-artificial-intelligence">EAAI</a>,
              <a href="https://www.sciencedirect.com/journal/journal-of-visual-communication-and-image-representation">JVCIR</a>,
              <a href="https://www.sciencedirect.com/journal/neurocomputing">Neurocomputing</a>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/WKU/WKU.png" style="width: 185px;"></td>
            <td width="75%" valign="center">
              <strong>Co-Instructor</strong> at <a href="https://www.wku.edu.cn/en/">Wenzhou-Kean University</a>

              <br>
              <br>
              <strong>Course</strong>: MATH 3291/3292 (Computer Vision)
              <br>
              <br>
              
              <a href="https://drive.google.com/drive/folders/1iYJHC2E_v-YYGZXK6BKJ543_o--p-RR4">Slide</a> | 
              <a href="https://drive.google.com/drive/folders/1ZiHHAlPxVj727sq7MyK9Lmpcusp3bip3">Recordings</a>
              
              <!-- <br>
              Collected state-of-the-art paper lists for paper reading, paper discussion, and literature review.  -->

              <!-- <br>
              I take the responsibility of 
              <a href="https://github.com/WKUAILAB/AI_Tutorial/tree/main/CV">Computer Vision</a>. -->
            </td>
          </tr>



      

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Services/Fudan.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong> Invited Speaker </strong> at <a href="https://www.fudan.edu.cn/en/">Fudan University</a>
              <br>
              <br>
              <strong>Topic</strong>: <a href="images/Services/Slides_Fudan.pdf">Image Processing with Machine Learning</a>
              <br>
            </td>
          </tr>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Services/Leetcode.jpg" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong> Content Creator</strong>: 
              <br>
              Made 100+ <a href="https://leetcode.com/">YouTube</a> video solutions for <a href="https://leetcode.com/">Leetcode</a> algorithms questions.
            </td>
          </tr>




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Skills</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Skills/Skills.jpeg" style="width: 185px;"></td>
            <td width="75%" valign="center">

              <strong>Programming Languages</strong>: 
              <br>
              Python, R, Java, C++, Matlab, HTML, Mathematica, Shell, LaTeX, Markdown
              <br>
              <br>

              <strong>Frameworks & Platforms</strong>: 
              <br>
              Pytorch, TensorFlow, Keras, Ubuntu, Docker, Git, ONNX, CUDA
              <br>
              <br>


              <strong>Libraries</strong>: 
              <br>
              Scikit-Learn, SciPy, NumPy, OpenCV, Matplotlib, Pandas
              <br>
              <br>
            </td>
          </tr>



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Fun Facts</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Fun_Facts/Fun_Facts.jpg" style="width: 185px;"></td>
              <td width="75%" valign="center">
  
                <strong>Languages</strong>: 
                <br>
                Chinese, English
                <br>
                <br>

                <strong>Sports</strong>: 
                <br>
                Basketball, Table Tennis, Swimming, Cycling, Hiking, Weightlifting
                <br>
                <br>

                <strong>Games</strong>: 
                <br>
                DOTA2, AOE2, Warcraft III
                <br>
                <br>

                <strong>Beliefs</strong>:
                <div style="display: flex; flex-wrap: wrap; gap: 5px;">
                  <a href="https://yunuoch.github.io/">‰∫àËØ∫‰∏âËßÇ,</a>
                  <a href="https://www.linkedin.com/in/haoran-qian/">Êµ©ÁÑ∂ÈÅìË∑Ø,</a>
                  <a href="https://mp.weixin.qq.com/s/EozCoQ2hSWjd27LJ8YwZCg">Á´ãÂ®ÅÊÄùÊÉ≥,</a>
                  <a href="https://www.linkedin.com/in/hang-xie/">Ë∞¢Ëà™Á≤æÁ•û,</a>
                  <a href="http://relic.wang/">Ê∞∏ÂØåÊñπÊ≥ï,</a>
                  <a href="https://www.linkedin.com/in/yuanzhe-liu-0a8a9a1b2/">ÂàòËøúÂì≤Â≠¶,</a>
                  <a href="https://a1600012888.github.io/">Â§©Ëøú‰ø°Âøµ,</a>
                  <a href="https://zpbao.github.io/">ÂøóÈπèÊô∫ÊÖß,</a>
                  <span>
                    <a href="https://www.linkedin.com/in/qiyuchen7/">Ê°Ü</a>-<a href="https://www.linkedin.com/in/junhong-zhou-7588b430a/">Âºò</a>-<a href="https://zh.wikipedia.org/zh-hans/%E5%86%85%E5%8D%B7%E5%8C%96">ÂÜÖÂç∑,</a>
                  </span>
                  <a href="https://www.linkedin.com/in/wanying-dou-540bb4207/">Â©âËéπÂøÉÊÄÅ,</a>
                  <span>
                    <a href="https://mwxinnn.github.io/about/">ËäØ</a>-<a href="https://scholar.google.com/citations?user=jPfElXcAAAAJ&hl=zh-CN">Áé≤</a>-<a href="http://www.guoxue.com/master/wangguowei/w-rjch01.htm">Â¢ÉÁïå</a>
                  </span>
                </div>


                <br>
              </td>
            </tr>
  

    </tr>
  </table>
</body>

</html>
