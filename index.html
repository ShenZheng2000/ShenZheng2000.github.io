<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shen Zheng</title>
  
  <meta name="author" content="Shen Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<head>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>

<body>

  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shen Zheng „ÄåÈÉëÊ∑±„Äç</name>
              </p>


              <p> I am an first-year PhD student at the Robotics Institute, Carnegie Mellon University <a href="https://www.cmu.edu/">(CMU)</a>, 
                working with <a href="http://www.cs.cmu.edu/~srinivas/"> Dr. Srinivasa Narasimhan</a>
              . Previously, I worked as a Perception Software Engineer at <a href="https://lucidmotors.com/">Lucid Motors</a>
                . I completed my Master of Science in Computer Vision (<a href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-computer-vision/">MSCV</a>) degree
                at CMU. Prior to joining CMU, I earned my bachelor's degree in Math from Wenzhou-Kean University  <a href="https://www.wku.edu.cn/">(WKU)</a>, where I worked
                with <a href="https://wku.edu.cn/faculty/gaurav-gupta/"> Dr. Gaurav Gupta </a>.

                <!-- on image-to-image translation and unsupervised domain adaptation for autonomous driving in adverse weather and lighting conditions.  -->
              </p>

              <p>
                Email1: shenzhen@andrew.cmu.edu
              </p>

              <p>
                Email2: lebronshenzheng@gmail.com
              </p>

              <p style="text-align:center">
                <!-- <a href="shenzhen@andrew.cmu.edu">Email</a> &nbsp/&nbsp -->
                <a href="images/Self/CV_ShenZheng.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=QV13_T8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ShenZheng2000">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shen-zheng-b05128184/">Linkedin</a> &nbsp/&nbsp 
                <a href="https://leetcode.com/LebronZheng/">Leetcode</a> &nbsp/&nbsp
                <a href="https://www.youtube.com/channel/UCwsy6n3UFfK2EavTfK1bI5w/playlists">YouTube</a>
              </p>
              
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Self/back.jpg"><img style="width:75%;max-width:75%" alt="profile photo" src="images/Self/back.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td id="research-areas" style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research Areas</heading>
                <p>
                  My current research focuses on improving domain adaptation for long-tail yet safety critical scenarios (e.g., bad weather, work zones) using the following strategies:
                </p>

                  <li><strong>Curate Images for Domain-Specific Feature Learning</strong>
                    <ul>
                      <li>Image-to-Image Translation: <a href="#tpsence">TPSeNCE</a></li>
                      <li>Image Collection: <a href="#roadwork">ROADWork</a></li>
                    </ul>
                  </li>
                  
                  <li><strong>Emphasize Salient Regions for Domain-Invariant Feature Learning</strong>
                    <ul>
                      <li>Image Warping: <a href="#instancewarp">Instance-Warp</a></li>
                    </ul>
                  </li>
                  
                  <p>
                    My earlier works focus on image restoration and enhancement, such as:
                  </p>
                  <ul>
                    <li>Low-Light Image Enhancement: <a href="#sgz">SGZ</a>, <a href="#lliesurvey">LLIE_Survey</a></li>
                    <li>Single Image Deraining: <a href="#sapnet">SAPNet</a></li>
                    <li>Blind Motion Deblurring: <a href="#debluryolo">Deblur-YOLO</a></li>


                </ul>
              </td>
            </tr>
          </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <!-- Additional content can go here if needed -->
          </tbody>
        </table>

 

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <p>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>




          <tr onmouseout="Road_Stop()" onmouseover="Road_start()" id="roadwork">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/RoadWork/roadwork.png" width="160"></div>
                <img src="images/RoadWork/roadwork.png" width="160">
              </div>
              <script type="text/javascript">
                function Road_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function Road_Stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                Road_Stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2207.06324"> -->
                <!-- <papertitle>Addressing Source Scale Bias via Instance-Level Image Warping for Domain Adaptation</papertitle> -->
                <papertitle>ROADWork Dataset: Learning to Recognize, Observe, Analyze and Drive Through Work Zones</papertitle>
              <!-- </a> -->
              <a href="#research-areas">
                <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
              </a>
              <br>
              Anurag Ghosh, <strong>Shen Zheng</strong>, Robert Tamburo, Juan R. Alvarez Padilla, Hailiang Zhu, Michael Cardei, Nicholas Dunn, Christoph Mertz, Srinivasa Narasimhan
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">Under Review </strong> 
              </em>
              <br>
              <a href="https://arxiv.org/abs/2406.07661">Paper</a> |
               <a href="http://www.cs.cmu.edu/~ILIM/roadwork_dataset/">Webpage</a>
              <br>
              <p>
                <strong>Motivation</strong>: 
                Navigating through work zones is challenging due to a lack of large-scale open datasets.
              </p>
              <p>
                <strong>Solution</strong>: 
                Introduce the ROADWork dataset, which is so far the largest open-source work zone dataset, to help learn how to recognize, observe, analyze, and drive through work zones.
              </p>
  
            </td>
          </tr>






          <tr onmouseout="Inst_stop()" onmouseover="Inst_start()" id="instancewarp">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/Inst_Warp/da-badweather.png" width="160"></div>
                <img src="images/Inst_Warp/da-badweather.png" width="160">
              </div>
              <script type="text/javascript">
                function Inst_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function Inst_stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                Inst_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2207.06324"> -->
                <!-- <papertitle>Addressing Source Scale Bias via Instance-Level Image Warping for Domain Adaptation</papertitle> -->
                <papertitle>Instance-Warp: Saliency Guided Image Warping for Unsupervised Domain Adaptation</papertitle>
              <!-- </a> -->
              <a href="#research-areas">
                <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
              </a>
              <br>
              <strong>Shen Zheng‚òÖ</strong>,
                    Anurag Ghosh‚òÖ,
                    Srinivasa Narasimhan
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">WACV 2025 </strong> 
              </em>
              <br>
              <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Zheng_Instance-Warp_Saliency_Guided_Image_Warping_for_Unsupervised_Domain_Adaptation_WACV_2025_paper.pdf">Paper</a> |
               <a href="https://instancewarp.github.io/">Webpage</a> |
               <a href="https://github.com/ShenZheng2000/Instance-Warp">Code</a>
              <br>
              <p>
                <strong>Motivation</strong>: 
                Domain adaptation methods struggle to learn smaller objects amidst dominant backgrounds with high cross-domain variations.
              </p>
              <p>
                <strong>Solution</strong>: 
                Warp source-domain images in-place using instance-level saliency to oversample objects and undersample backgrounds during domain adaptation training.
              </p>
            </td>
          </tr>




          <tr onmouseout="TPSeNCE_stop()" onmouseover="TPSeNCE_start()" id="tpsence">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/TPSeNCE/Titlepage.png" width="160"></div>
                <img src='images/TPSeNCE/Titlepage.png' width="160">
              </div>
              <script type="text/javascript">
                function TPSeNCE_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function TPSeNCE_stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                TPSeNCE_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2207.06324"> -->
                <papertitle>TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain</papertitle>
              <!-- </a> -->
              <a href="#research-areas">
                <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
              </a>
              <br>
              <strong>Shen Zheng</strong>,
                    Changjie Lu,
                    Srinivasa Narasimhan
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">WACV 2024</strong> 
              </em>
              <br>
              <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Zheng_TPSeNCE_Towards_Artifact-Free_Realistic_Rain_Generation_for_Deraining_and_Object_WACV_2024_paper.pdf">Paper</a> |
              <a href="https://shenzheng2000.github.io/tpsence.github.io/">Webpage</a> |
              <a href="https://github.com/ShenZheng2000/TPSeNCE/">Code</a> |
              <!-- <a href="https://openaccess.thecvf.com/content/WACV2024/supplemental/Zheng_TPSeNCE_Towards_Artifact-Free_WACV_2024_supplemental.pdf">Supp</a> | -->
              <a href="images/TPSeNCE/TPSeNCE_Slides.pdf">Slides</a> |
              <a href="images/TPSeNCE/TPSeNCE_Poster.pdf">Poster</a>
              <p>
                <strong>Motivation</strong>: 
                Previous image-to-image translation methods produce artifacts and distortions, and lack control over the amount of rain generated. 
              </p>
              <p>
                <strong>Solution</strong>: 
                Introduce a Triangular Probability Similarity (TPS) loss to minimize the artifacts and distortions during rain generation.  
                Propose a Semantic Noise Contrastive Estimation (SeNCE) strategy to optimize the amounts of generated rain. 
                Show that realistic rain generation benefits deraining and object detection in rain. 
              </p>
  
            </td>
          </tr>
      

          <tr onmouseout="Point_cloud_stop()" onmouseover="Point_cloud_start() " id="pointnorm">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/PointNorm/PointNorm.png" width="160"></div>
                <img src='images/PointNorm/PointNorm.png' width="160">
              </div>
              <script type="text/javascript">
                function Point_cloud_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function Point_cloud_stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                Point_cloud_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2207.06324"> -->
                <papertitle>PointNorm: Dual Normalization is All You Need for Point Cloud Analysis</papertitle>
              <!-- </a> -->
              <a href="#research-areas">
                <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
              </a>
              <br>
              <strong>Shen Zheng</strong>,
              Jinqian Pan,
              Changjie Lu,
              Gaurav Gupta
              <br>
              <!-- <em>IJCNN 2023</em> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">IJCNN 2023 (Oral Presentation)</strong> 
              </em>
              <br>
              <a href="https://arxiv.org/abs/2207.06324">Paper</a> |
              <a href="https://shenzheng2000.github.io/pointnorm.github.io/">Webpage</a> |
              <a href="https://github.com/ShenZheng2000/PointNorm-for-Point-Cloud-Analysis">Code</a> |
              <a href="images/PointNorm/IJCNN_2023_Pre.pdf">Slides</a> 
              <p>
                <strong>Motivation</strong>: Current point cloud analysis methods struggles with irregular (i.e., unevenly distributed) point clouds. 
              </p>
              <p>
                <strong>Solution</strong>: PointNorm, a point cloud analysis network with a DualNorm module (Point Normalization & Reverse Point Normalization) that leverages local mean and global standard deviation.
              </p>
  
            </td>
          </tr>
  


        <tr onmouseout="LLIE_stop()" onmouseover="LLIE_start()" id="sgz">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='night'>
              <img src="images/SGZ/F1.png" width="160"></div>
              <img src='images/SGZ/F1Crop.png' width="160">
            </div>
            <script type="text/javascript">
              function LLIE_start() {
                document.getElementById('night').style.opacity = "0";
              }

              function LLIE_stop() {
                document.getElementById('night').style.opacity = "1";
              }
              LLIE_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <a href="images/SGZ/SGZ.pdf"> -->
              <papertitle>Semantic-Guided Zero-Shot Learning for Low-Light Image/Video Enhancement</papertitle>
            <!-- </a> -->
            <a href="#research-areas">
              <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
            </a>
            <br>
            <strong>Shen Zheng</strong>, 
            Gaurav Gupta
            <br>
            <em style="display: block; margin: 10px 0 -5px;"> 
              <strong style="color: red;">WACV 2022</strong> 
            </em>
            <br>
            <a href="https://arxiv.org/pdf/2110.00970.pdf">Paper</a> |
            <a href="https://shenzheng2000.github.io/sgz.github.io/">Webpage</a> |
            <!-- <a href="https://openaccess.thecvf.com/content/WACV2022W/RWS/supplemental/Zheng_Semantic-Guided_Zero-Shot_Learning_WACVW_2022_supplemental.pdf">Supp</a> | -->
            <a href="https://github.com/ShenZheng2000/Semantic-Guided-Low-Light-Image-Enhancement">Code</a> |
            <a href="images/SGZ/SGZ_Slides.pdf">Slides</a>
            <p>
              <strong>Motivation</strong>: Current low-light image enhancement methods cannot handle uneven illuminations, is computationally inefficient, and fail to preserve the semantic information. 
            </p>
            <p>
              <strong>Solution</strong>: Introduce SGZ, a zero-shot low-light image enhancement framework with pixel-wise light deficiency estimation, parameter-free recurrent image enhancement, and unsupervised semantic segmentation.
            </p>
          </td>
        </tr>


        
        <tr onmouseout="ACML_stop()" onmouseover="ACML_start()"  id="asintrovae">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='acml'>
              <img src="images/AS_IntroVAE/AS_IntroVAE.JPG" width="160"></div>
              <img src='images/AS_IntroVAE/AS_IntroVAE.JPG' width="160">
            </div>
            <script type="text/javascript">
              function ACML_start() {
                document.getElementById('acml').style.opacity = "1";
              }

              function ACML_stop() {
                document.getElementById('acml').style.opacity = "0";
              }
              UDA_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <a href="images/AS_IntroVAE/AS_IntroVAE_Paper.pdf"> -->
              <papertitle>AS-IntroVAE: Adversarial Similarity Distance Makes Robust IntroVAE</papertitle>
            <!-- </a> -->
            <a href="#research-areas">
              <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
            </a>
            <br>
            Changjie Lu, 
            <strong>Shen Zheng</strong>, 
            Zirui Wang,
            Omar Dib,
            Gaurav Gupta
            <br>
            <em style="display: block; margin: 10px 0 -5px;"> 
              <strong style="color: red;">ACML 2022</strong> 
            </em> 
            <br>
            <!-- <a href="">code</a> / -->
            <a href="https://arxiv.org/pdf/2206.13903.pdf">Paper</a> |            
            <a href="https://github.com/ShenZheng2000/SAPNet-for-image-deraining">Code</a> |
            <a href="images/AS_IntroVAE/AS_IntroVAE_Slides.pdf">Slides</a>
            <p>
              <strong>Motivation</strong>: Generative models experience posterior collapse and vanishing gradient due to no effective metric for real-fake image evaluation.
            </p>
            <p>
              <strong>Solution</strong>: Propose Adversarial Similarity Distance Introspective Variational Autoencoder (AS-IntroVAE), which can address the posterior
              collapse and the vanishing gradient problem in image generation in one go. 
            </p>
          </td>
        </tr>




        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Experiences</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Lucid/images.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong>Perception Software Engineer (Full-Time)</strong> at <a href="https://lucidmotors.com/">Lucid Motors</a>

              <p>
                Director: <a href="https://www.linkedin.com/in/feng-guo-7126594/">Dr. Feng Guo</a>
              </p>

              <p>
                Working as a perception software engineer in the ADAS perception team responsible for auto-parking, traffic light detection, and blockage detection.
            </p>

            <p>
              Improved BEVFormer for auto-parking (reverse & parallel) by using extrinsic calibration to interpolate and smooth edges to enhance curb detection.
            </p>

            <p>
              Trained YOLO6 on full-resolution images containing traffic lights and fine-tuned arrow types, confidence, IoU, and area thresholds, resulting in a 40+% improvement in mAP (final mAP: 98%+ for day; 90%+ for night).
            </p>

            <p>
              Developed a binary semantic segmentation model based on CenterNet to detect blockages such as ice, snow, mud, mud blur, rain drops, and sun glares, achieving 93%+ IoU. 
            </p>

            <!-- <p>
              Improved battery template matching to ensure success across all captured battery images, including in low-light or overexposed conditions.
            </p> -->


            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Momenta/Momenta.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong>Perception Engineer (Intern) </strong> at <a href="https://www.momenta.cn/">Momenta</a>

              <p>
                Director: <a href="https://scholar.google.com.sg/citations?user=Wo5Cem4AAAAJ&hl=en">Dr. Wangjiang Zhu</a>
                 
              </p>

              <p>
              Responsible for long-tailed data augmentation, training data auto-labeling and cleaning, and model evaluation for traffic light detection algorithms.
            </p>

            <p>
              Implemented CycleGAN to conduct unsupervised data augmentation, converting traffic light bulbs from left arrow to round & leftUturn arrow.
            </p>

            <p>
              Constructed a traffic light auto-label model using quantized VoVNet-57, filtering 14,618 incorrect annotations from 1,160,513 labeled frames.
            </p>

            <p>
              Increased the classification accuracy for leftUturn traffic light from 78.41% to 87.27%, and the mean average precision from 93.01% to 94.80%.
            </p>
            </td>
          </tr>
 
    




          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Other Publications</heading>
              <p>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="LLIE_Survey_stop()" onmouseover="LLIE_Survey_start()"  id="lliesurvey">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/LLIE_Survey/Timeline.png" width="160"></div>
                <img src='images/LLIE_Survey/Timeline.png' width="160">
              </div>
              <script type="text/javascript">
                function LLIE_Survey_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function LLIE_Survey_stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                LLIE_Survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2207.06324"> -->
                <papertitle>Low-Light Image Enhancement: A Comprehensive Survey and Beyond</papertitle>
              <!-- </a> -->
              <a href="#research-areas">
                <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
              </a>
              <br>
              <strong>Shen Zheng</strong>,
              Yiling Ma,
              Jinqian Pan,
              Changjie Lu,
              Gaurav Gupta
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;"></strong> 
              </em>
              <br>
              <a href="https://arxiv.org/abs/2212.10772">Paper</a> |
              <a href="https://github.com/ShenZheng2000/LLIE_Survey">Code</a>
              <p>
                <strong>Motivation</strong>: 
                Existing LLIE datasets focus on either overexposure or underexposure, not both, and usually feature minimally degraded images captured from static positions.
              </p>
              <p>
                <strong>Solution</strong>: 
                Present a comprehensive survey of low-light image enhancement (LLIE).
                Propose the SICE_Grad and SICE_Mix image datasets, which include images with both overexposure and underexposure. 
                Introduce Night Wenzhou, a large-scale, high-resolution video dataset captured in fast motion with diverse illuminations and degradation.
              </p>
  
            </td>
          </tr>





        <tr onmouseout="rain_stop()" onmouseover="rain_start()" id="sapnet">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='rainy'>
              <img src="images/SAPNet/rainy.png" width="160"></div>
              <img src='images/SAPNet/derain.png' width="160">
            </div>
            <script type="text/javascript">
              function rain_start() {
                document.getElementById('rainy').style.opacity = "0";
              }
  
              function rain_stop() {
                document.getElementById('rainy').style.opacity = "1";
              }
              rain_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <a href="https://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Zheng_SAPNet_Segmentation-Aware_Progressive_Network_for_Perceptual_Contrastive_Deraining_WACVW_2022_paper.pdf"> -->
              <papertitle>SAPNet: Segmentation-Aware Progressive Network for Perceptual Contrastive Deraining</papertitle>
            <!-- </a> -->
            <a href="#research-areas">
              <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
            </a>
            <br>
            <strong>Shen Zheng</strong>, 
            Changjie Lu, 
            Yuxiong Wu, 
            Gaurav Gupta
            <br>
            <em style="display: block; margin: 10px 0 -5px;"> 
              <strong style="color: red;">WACVW 2022</strong> 
            </em>
            <br>
            <a href="https://arxiv.org/abs/2111.08892">Paper</a> |
            <!-- <a href="https://openaccess.thecvf.com/content/WACV2022W/VAQ/supplemental/Zheng_SAPNet_Segmentation-Aware_Progressive_WACVW_2022_supplemental.pdf">Supp</a> | -->
            <a href="https://github.com/ShenZheng2000/SAPNet-for-image-deraining">Code</a> |
            <a href="images/SAPNet/SAPNet_Slides.pdf">Slides</a>
            <!-- <a href="https://www.youtube.com/watch?v=tJSsICHpsfs">Video</a> -->
            <p>
              <strong>Motivation</strong>: Former deraining approaches often eliminate essential background details along with the rain, hindering tasks such as detection and segmentation. 
            </p>
              <p>
                <strong>Solution</strong>: SAPNet, an image-deraining network that integrates low-level image-deraining and high-level background segmentation using progressive dilated unit, perceptual contrastive loss, and unsupervised background segmentation.
            </p>
          </td>
        </tr>
  
  

 
            <!-- Maybe: Use images, not figures -->
              <tr onmouseout="UDA_stop()" onmouseover="UDA_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='UDA_VAE++'>
                    <img src="images/UDA_VAE++/UDA_VAE++.png" width="160"></div>
                    <img src='images/UDA_VAE++/UDA_VAE++.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function UDA_start() {
                      document.getElementById('mine').style.opacity = "1";
                    }
    
                    function UDA_stop() {
                      document.getElementById('mine').style.opacity = "0";
                    }
                    UDA_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="https://arxiv.org/pdf/2204.09334.pdf"> -->
                    <papertitle>Unsupervised Domain Adaptation for Cardiac Segmentation: Towards Structure
                      Mutual Information Maximization</papertitle>
                  <!-- </a> -->
                  <a href="#research-areas">
                    <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
                  </a>
                  <br>
                  Changjie Lu, 
                  <strong>Shen Zheng</strong>, 
                  Gaurav Gupta
                  <br>
                  <em style="display: block; margin: 10px 0 -5px;"> 
                    <strong style="color: red;">CVPRW 2022</strong> 
                  </em> 
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Lu_Unsupervised_Domain_Adaptation_for_Cardiac_Segmentation_Towards_Structure_Mutual_Information_CVPRW_2022_paper.pdf">Paper</a> |
                  <!-- <a href="https://openaccess.thecvf.com/content/CVPR2022W/Precognition/supplemental/Lu_Unsupervised_Domain_Adaptation_CVPRW_2022_supplemental.pdf">Supp</a> | -->
                  <a href="https://github.com/LOUEY233/Toward-Mutual-Information">Code</a> |
                  <a href="images/UDA_VAE++/UDA_VAE++.pdf">Slides</a> 
    
                  <p>
                    <strong>Motivation</strong>: Previous unsupervised domain adaptation methods for medical imaging falter across varied imaging modalities due to substantial domain differences.
                  </p>
                  <p>
                    <strong>Solution</strong>: UDA-VAE++, an unsupervised domain adaptation framework that leverages mutual information maximization and sequential reparameterization for cardiac segmentation.
                  </p>
                </td>
              </tr>
    
    
              <tr onmouseout="ijcnn_stop()" onmouseover="ijcnn_start()" id="debluryolo">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='blur'>
                    <img src="images/Deblur_YOLO/baby_blur.png" width="160"></div>
                    <img src='images/Deblur_YOLO/baby_deblurYolo.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function ijcnn_start() {
                      document.getElementById('blur').style.opacity = "0";
                    }
    
                    function ijcnn_stop() {
                      document.getElementById('blur').style.opacity = "1";
                    }
                    ijcnn_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="https://ieeexplore.ieee.org/document/9534352/"> -->
                    <papertitle>Deblur-YOLO: Real-Time Object Detection with Efficient Blind Motion Deblurring</papertitle>
                  <!-- </a> -->
                  <a href="#research-areas">
                    <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
                  </a>
                  <br>
                  <strong>Shen Zheng</strong>, 
                  Yuxiong Wu,
                  Shiyu Jiang,
                  Changjie Lu, 
                  Gaurav Gupta
                  <br>
                  <em style="display: block; margin: 10px 0 -5px;"> 
                    <strong style="color: red;">IJCNN 2021 (Oral Presentation)</strong> 
                  </em> 
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9534352/">Paper</a> |
                  <a href="images/Deblur_YOLO/Deblur_YOLO_Slides.pdf">Slides</a>
                  <p>
                    <strong>Motivation</strong>: Object detection algorithms exhibit suboptimal performance on blurry scenes.
                  </p>
                  <p>
                    <strong>Solution</strong>: Propose Deblur-YOLO, a generative adversarial network with a dilated feature pyramid generator, double multi-scale discriminators, and a detection discriminator
                  to deal with photographs corrupted by motion blur.
                  </p>
                </td>
              </tr>
    
              <tr onmouseout="EES_stop()" onmouseover="EES_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='EES'>
                    <img src="images/EES/EES.png" width="160"></div>
                    <img src='images/EES/EES.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function EES_start() {
                      document.getElementById('EES').style.opacity = "1";
                    }
    
                    function EES_stop() {
                      document.getElementById('EES').style.opacity = "0";
                    }
                    UDA_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="images/AS_IntroVAE/AS_IntroVAE_Paper.pdf"> -->
                    <papertitle>Efficient Ensemble Sparse Convolutional Neural Networks with Dynamic Batch Size</papertitle>
                  <!-- </a> -->
                  <a href="#research-areas">
                    <i class="fa-solid fa-house"></i> <!-- This adds a left arrow icon -->
                  </a>
                  <br>
                  <strong>Shen Zheng</strong>, 
                  Liwei Wang,
                  Gaurav Gupta
                  <br>
                  <em style="display: block; margin: 10px 0 -5px;"> 
                    <strong style="color: red;">CVIP 2020 (Oral Presentation)</strong> 
                  </em> 
                  <br>
                  <!-- <a href="">code</a> / -->
                  <a href="https://link.springer.com/chapter/10.1007/978-981-16-1103-2_23">Paper</a> |
                  <a href="images/EES/EES_Slides.pdf">Slides</a>
                  <p>
                    <strong>Motivation</strong>: Existing ConvNets have poor computational complexity and require significant memory consumption.
                  </p>  
                  <p>
                    <strong>Solution</strong>: Introduce an efficient ConvNet with weighted average stacking, Winograd-ReLU-based network pruning, and a electromagnetic-inspired dynamic batch size algorithm.
                  </p>
                </td>
              </tr>
    


   



<!--             
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/UND/UND.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong> Research Assistant </strong> at <a href="https://www.nd.edu/">University of Notre Dame</a>

              <p>
                Supervisor: <a href="https://sites.nd.edu/chaoli-wang/">Dr. Chaoli Wang</a>
                 
              </p>

              <p>
              Developed a fully convolutional neural network with Siren activation function to render isosurfaces with image resolution, viewpoints and isovalue.
            </p>

              <p>
              Leveraged Greene's bisection method and Jacobian matrix's eigenvalue for critical point detection and classification in the simulated 3D isosurface.
            </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/China_Life/China_Life.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong>Sales Analyst Intern </strong> at <a href="https://www.chinalife.com.hk/zh-cn">China Life insurance Company</a>

              <p>
                Applied K-means clustering model to classify text data statewide as three significant categories to eliminate the risk from over 20,000 unannounced expired insurance from 7 cities.
              </p>

              <p>
                Employed t-test and Adjusted R Squared to help Sales Manager and General Manager deciding the bonus percentage for consecutive monthly sales as 6.00%.
              </p>

            </td>
          </tr> -->





        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Services/CVF.jpg" style="width: 185px;"></td>
            <td width="75%" valign="center">

              <strong>Technical Program Committee</strong>: 
              <br>
              <a href="https://2024.ieeewcci.org/">WCCI 2024</a>
              <br>
              <br>

              <strong>Conference Reviewers</strong>: 
              <br>
              CVIP (<a href="https://sites.google.com/view/cvip-2021/home">2021</a>, <a href="https://vnit.ac.in/cvip2022/">2022</a>), 
              AAAI (<a href="https://aaai.org/Conferences/AAAI-22/">2022</a>), 
              IJCNN (<a href="https://2023.ijcnn.org/">2023</a>, <a href="https://2024.ijcnn.org/">2024</a>, <a href="https://2025.ijcnn.org/">2025</a>), 
              WACV (<a href="https://wacv2023.thecvf.com/home">2023</a>, <a href="https://wacv2024.thecvf.com/home">2024</a>, <a href="https://wacv2025.thecvf.com/home">2025</a>), 
              ECCV (<a href="https://eccv2024.ecva.net/">2024</a>), 
              CVPR (<a href="https://2024.ijcnn.org/">2025</a>),
              ICCV (<a href="https://iccv.thecvf.com/">2025</a>)
              <br>
              <br>

              <strong>Journal Reviewers</strong>:
              <br>
              <a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems">TNNLS</a>,
              <a href="https://www.springer.com/journal/11263">IJCV</a>,
              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">TCSVT</a>


            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/WKU/WKU.png" style="width: 185px;"></td>
            <td width="75%" valign="center">
              <strong>Co-Instructor</strong> at <a href="https://www.wku.edu.cn/en/">Wenzhou-Kean University</a>

              <br>
              <br>
              <strong>Course</strong>: MATH 3291/3292 (Computer Vision)
              <br>
              <br>
              
              <a href="https://drive.google.com/drive/folders/1iYJHC2E_v-YYGZXK6BKJ543_o--p-RR4">Slide</a> | 
              <a href="https://drive.google.com/drive/folders/1ZiHHAlPxVj727sq7MyK9Lmpcusp3bip3">Recordings</a>
              
              <!-- <br>
              Collected state-of-the-art paper lists for paper reading, paper discussion, and literature review.  -->

              <!-- <br>
              I take the responsibility of 
              <a href="https://github.com/WKUAILAB/AI_Tutorial/tree/main/CV">Computer Vision</a>. -->
            </td>
          </tr>



      

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Services/Fudan.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong> Invited Speaker </strong> at <a href="https://www.fudan.edu.cn/en/">Fudan University</a>
              <br>
              <br>
              <strong>Topic</strong>: <a href="images/Services/Slides_Fudan.pdf">Image Processing with Machine Learning</a>
              <br>
            </td>
          </tr>


  
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Services/AI_Lab.jpg" style="width: 185px;"></td>
            <td width="75%" valign="center">
              <strong>Co-founder</strong> of <a href="https://github.com/WKUAILAB">WKU AI-LAB</a>

              <br>
              <br>
              Offered AI Tutorials in Computer Vision and Natural Language Processing for undegraduate students. 
              <br>
              
              <!-- <br>
              Collected state-of-the-art paper lists for paper reading, paper discussion, and literature review.  -->

              <!-- <br>
              I take the responsibility of 
              <a href="https://github.com/WKUAILAB/AI_Tutorial/tree/main/CV">Computer Vision</a>. -->
            </td>
          </tr>

                      
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Services/Leetcode.jpg" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong> Content Creator</strong>: 
              <br>
              Made 100+ <a href="https://leetcode.com/">YouTube</a> video solutions for <a href="https://leetcode.com/">Leetcode</a> algorithms questions.
            </td>
          </tr>




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Skills</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Skills/Skills.jpeg" style="width: 185px;"></td>
            <td width="75%" valign="center">
              <!-- <a href="https://github.com/WKUAILAB">WKU AI-LAB</a> -->

              <strong>Programming Languages</strong>: 
              <br>
              Python, R, Java, C++, Matlab, HTML, Mathematica, Shell, LaTeX, Markdown
              <br>
              <br>

              <strong>Frameworks & Platforms</strong>: 
              <br>
              Pytorch, TensorFlow, Keras, Ubuntu, Docker, Git, ONNX, CUDA
              <br>
              <br>


              <strong>Libraries</strong>: 
              <br>
              Scikit-Learn, SciPy, NumPy, OpenCV, Matplotlib, Pandas
              <br>
              <br>
            </td>
          </tr>



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Fun Facts</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Fun_Facts/Fun_Facts.jpg" style="width: 185px;"></td>
              <td width="75%" valign="center">
                <!-- <a href="https://github.com/WKUAILAB">WKU AI-LAB</a> -->
  
                <strong>Languages</strong>: 
                <br>
                Chinese, English
                <br>
                <br>

                <strong>Sports</strong>: 
                <br>
                Basketball, Table Tennis, Swimming, Cycling, Hiking, Weightlifting
                <br>
                <br>

                <strong>Games</strong>: 
                <br>
                DOTA2, AOE2, Warcraft III
                <br>
                <br>

                <strong>Beliefs</strong>: 
                <div style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis;">
                  <a href="https://yunuoch.github.io/">‰∫àËØ∫‰∏âËßÇ</a> & 
                  <a href="https://www.linkedin.com/in/haoran-qian/">Êµ©ÁÑ∂ÈÅìË∑Ø</a> & 
                  <a href="https://www.researchgate.net/profile/Liwei-Wang-15">Á´ãÂ®ÅÊÄùÊÉ≥</a> & 
                  <a href="https://www.linkedin.com/in/hang-xie/">Ë∞¢Ëà™Á≤æÁ•û</a> & 
                  <a href="http://relic.wang/">Ê∞∏ÂØåÊñπÊ≥ï</a> & 
                  <a href="https://www.linkedin.com/in/yuanzhe-liu-0a8a9a1b2/">ÂàòËøúÂì≤Â≠¶</a> & 
                  <a href="https://a1600012888.github.io/">Â§©Ëøú‰ø°Âøµ</a> & 
                  <a href="https://zpbao.github.io/">ÂøóÈπèÊô∫ÊÖß</a> & 
                  <a href="https://www.linkedin.com/in/qiyuchen7/">Ê°Ü</a>
                  <a href="https://www.linkedin.com/in/junhong-zhou-7588b430a/">Âºò</a>
                  <a href="https://paperswithcode.com/method/involution">ÂÜÖÂç∑</a>

                </div>

                <!-- <strong>Hates</strong>:
                <br>
                Ê∂àË¥π‰∏ª‰πâ &
                ‰∫´‰πê‰∏ª‰πâ &
                ÈÄÉË∑ë‰∏ª‰πâ &
                ÊÆñÊ∞ë‰∏ª‰πâ &
                ÁßçÊóè‰∏ª‰πâ &
                Èú∏ÊùÉ‰∏ª‰πâ &
                ÂÜõÂõΩ‰∏ª‰πâ 
                <br> -->


                <br>
              </td>
            </tr>
  

<!-- 
          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Visitors</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.revolvermaps.com/livestats/5xcf5hcpcuz/"><img src="//rf.revolvermaps.com/h/m/a/0/ff0000/128/0/5xcf5hcpcuz.png" width="256" height="128" alt="Map" style="border:0;"></a>      </td>
            </td>
          </tr> -->


    </tr>
  </table>
</body>

</html>
