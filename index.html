<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shen Zheng</title>
  
  <meta name="author" content="Shen Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>


  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shen Zheng</name>
              </p>

              <p>I am a Master of Science in Computer Vision (<a href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-computer-vision/">MSCV</a>) student 
                at Carnegie Mellon University <a href="https://www.cmu.edu/">(CMU)</a>, working with <a href="http://www.cs.cmu.edu/~srinivas/"> Dr. Srinivasa Narasimhan </a> 
                at Illumination and Imaging Lab <a href="https://www.cs.cmu.edu/~ILIM/"> (LLIM)</a>. 
                Before joining CMU, I obtained  my bachelor's degree in Math from Wenzhou-Kean University  <a href="https://www.wku.edu.cn/">(WKU)</a>, where I worked
                with <a href="https://wku.edu.cn/faculty/gaurav-gupta/"> Dr. Gaurav Gupta </a>.

                <!-- on image-to-image translation and unsupervised domain adaptation for autonomous driving in adverse weather and lighting conditions.  -->
              </p>

              <p>
                Email: shenzhen@andrew.cmu.edu
              </p>

              <p><strong>I am looking for a Ph.D. position starting Fall 2024!</strong></p>

              <!-- <p>
               During my undergrad, I have worked with 
                on efficient network training, image restoration & enhancement & generation, and point cloud analysis.
              </p> -->

              <!-- <p>
                I have also attended the <a href="https://ndi-sa.nd.edu/index.cfm?FuseAction=Programs.ViewProgramAngular&id=10096">iSURE</a> program 
                provided by the <a href="https://www.nd.edu/">University of Notre Dame</a>,
                where I worked with <a href="https://sites.nd.edu/chaoli-wang/">Dr. Chaoli Wang</a> on implicit representation for isosurface rendering. 
              </p> -->

                <!-- <p>
                  Besides, I have worked in the perception team in <a href="https://www.momenta.cn/">Momenta</a>, where I was responsible for 
                  data augmentation, data autolabeling, data cleaning, and object detection for traffic lights. 
                </p> -->
                

                <!-- <p>If interested in research collaboration, feel free to drop me an email (shenzhen@andrew.cmu.edu).</p> -->

              <p style="text-align:center">
                <!-- <a href="shenzhen@andrew.cmu.edu">Email</a> &nbsp/&nbsp -->
                <a href="images/Self/CV_ShenZheng.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=QV13_T8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ShenZheng2000">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shen-lebron-zheng-b05128184/">Linkedin</a> &nbsp/&nbsp 
                <a href="https://leetcode.com/LebronZheng/">Leetcode</a> &nbsp/&nbsp
                <a href="https://www.youtube.com/channel/UCwsy6n3UFfK2EavTfK1bI5w/playlists">YouTube</a>
              </p>
              
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Self/Image_ShenZheng.jpg"><img style="width:75%;max-width:75%" alt="profile photo" src="images/Self/Image_ShenZheng.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Research Areas</heading>
            <p>
              My research focus is better scene understanding across diverse environments using the following strategies:
            </p>
            <ul>
              <li><strong>Preprocessing</strong>: Blind Motion Deblurring, Single Image Deraining, Low-Light Image Enhancement</li>
              <li><strong>Finetuning</strong>: Image Generation, Image-to-Image Translation</li>
              <li><strong>Domain Adaptation</strong>: Image Warping, Mutual Information Estimation</li>
            </ul>

          
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

 



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Conference Papers</heading>
              <p>
                <!-- Maybe: summarize main research points -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



          <tr onmouseout="Inst_stop()" onmouseover="Inst_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/Inst_Warp/Draw-1.png" width="160"></div>
                <img src="images/Inst_Warp/Draw-1.png" width="160">
              </div>
              <script type="text/javascript">
                function Inst_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function Inst_stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                Inst_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2207.06324"> -->
                <papertitle>Addressing Source Scale Bias via Instance-Level Image Warping for Domain Adaptation</papertitle>
              <!-- </a> -->
              <br>
              <strong>Shen Zheng</strong>,
                    Anurag Ghosh,
                    Srinivasa Narasimhan
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">Under Anonymous Review </strong> 
              </em>
              <br>
              <p>
                <strong>Motivation</strong>: 
                Object Scale bias challenges contemporal visual recognition systems.
              </p>
              <p>
                <strong>Solution</strong>: 
                Propose a instance-level image warping technique using dataset-specific size statistics to warp images in-place during training to address object scale bias.
                Integrate image warping and feature unwarping into domain adaptation in a task-agnostic way without warping at test time. 
              </p>
  
            </td>
          </tr>





          <tr onmouseout="TPSeNCE_stop()" onmouseover="TPSeNCE_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/TPSeNCE/Titlepage.png" width="160"></div>
                <img src='images/TPSeNCE/Titlepage.png' width="160">
              </div>
              <script type="text/javascript">
                function TPSeNCE_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function TPSeNCE_stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                TPSeNCE_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2207.06324"> -->
                <papertitle>TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain</papertitle>
              <!-- </a> -->
              <br>
              <strong>Shen Zheng</strong>,
                    Changjie Lu,
                    Srinivasa Narasimhan
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">WACV 2024</strong> 
              </em>
              <br>
              <a href="https://arxiv.org/pdf/2311.00660.pdf">Paper</a> |
              <a href="https://github.com/ShenZheng2000/TPSeNCE/">Code</a> |
              <a href="https://www.youtube.com/watch?v=eNS_8fuSLjc">Video</a>
              <p>
                <strong>Motivation</strong>: 
                Previous image-to-image translation methods produce artifacts and distortions, and lack control over the amount of rain generated. 
              </p>
              <p>
                <strong>Solution</strong>: 
                Introduce a Triangular Probability Similarity (TPS) loss to minimize the artifacts and distortions during rain generation.  
                Propose a Semantic Noise Contrastive Estimation (SeNCE) strategy to optimize the amounts of generated rain. 
                Evaluate rain generation performances using rain removal and object detection. 
              </p>
  
            </td>
          </tr>
      

          <tr onmouseout="LLIE_Survey_stop()" onmouseover="LLIE_Survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv'>
                <img src="images/LLIE_Survey/Timeline.png" width="160"></div>
                <img src='images/LLIE_Survey/Timeline.png' width="160">
              </div>
              <script type="text/javascript">
                function LLIE_Survey_start() {
                  document.getElementById('wacv').style.opacity = "1";
                }
  
                function LLIE_Survey_stop() {
                  document.getElementById('wacv').style.opacity = "0";
                }
                LLIE_Survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/abs/2207.06324"> -->
                <papertitle>Low-Light Image Enhancement: A Comprehensive Survey and Beyond</papertitle>
              <!-- </a> -->
              <br>
              <strong>Shen Zheng</strong>,
              Yiling Ma,
              Jinqian Pan,
              Changjie Lu,
              Gaurav Gupta
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">TNNLS</strong> 
              </em>
              <br>
              <a href="https://arxiv.org/abs/2212.10772">Paper</a> |
              <a href="https://github.com/ShenZheng2000/LLIE_Survey">Code</a>
              <p>
                <strong>Motivation</strong>: 
                Existing LLIE datasets focus on either overexposure or underexposure, not both, and usually feature minimally degraded images captured from static positions.
              </p>
              <p>
                <strong>Solution</strong>: 
                Present a comprehensive survey of low-light image enhancement (LLIE).
                Propose the SICE_Grad and SICE_Mix image datasets, which include images with both overexposure and underexposure. 
                Introduce Night Wenzhou, a large-scale, high-resolution video dataset captured in fast motion with diverse illuminations and degradation.
              </p>
  
            </td>
          </tr>




        <tr onmouseout="Point_cloud_stop()" onmouseover="Point_cloud_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='wacv'>
              <img src="images/PointNorm/PointNorm.png" width="160"></div>
              <img src='images/PointNorm/PointNorm.png' width="160">
            </div>
            <script type="text/javascript">
              function Point_cloud_start() {
                document.getElementById('wacv').style.opacity = "1";
              }

              function Point_cloud_stop() {
                document.getElementById('wacv').style.opacity = "0";
              }
              Point_cloud_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <a href="https://arxiv.org/abs/2207.06324"> -->
              <papertitle>PointNorm: Dual Normalization is All You Need for Point Cloud Analysis</papertitle>
            <!-- </a> -->
            <br>
            <strong>Shen Zheng</strong>,
            Jinqian Pan,
            Changjie Lu,
            Gaurav Gupta
            <br>
            <!-- <em>IJCNN 2023</em> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
            <em style="display: block; margin: 10px 0 -5px;"> 
              <strong style="color: red;">IJCNN 2023 (Oral Presentation)</strong> 
            </em>
            <br>
            <a href="https://arxiv.org/abs/2207.06324">Paper</a> |
            <a href="https://github.com/ShenZheng2000/PointNorm-for-Point-Cloud-Analysis">Code</a> |
            <a href="images/PointNorm/IJCNN_2023_Pre.pdf">Slides</a> 
            <p>
              <strong>Motivation</strong>: Current point cloud analysis methods struggles with irregular (i.e., unevenly distributed) point clouds. 
            </p>
            <p>
              <strong>Solution</strong>: PointNorm, a point cloud analysis network with a DualNorm module (Point Normalization & Reverse Point Normalization) that leverages local mean and global standard deviation.
            </p>

          </td>
        </tr>

        <tr onmouseout="LLIE_stop()" onmouseover="LLIE_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='night'>
              <img src="images/SGZ/F1.png" width="160"></div>
              <img src='images/SGZ/F1Crop.png' width="160">
            </div>
            <script type="text/javascript">
              function LLIE_start() {
                document.getElementById('night').style.opacity = "0";
              }

              function LLIE_stop() {
                document.getElementById('night').style.opacity = "1";
              }
              LLIE_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <a href="images/SGZ/SGZ.pdf"> -->
              <papertitle>Semantic-Guided Zero-Shot Learning for Low-Light Image/Video Enhancement</papertitle>
            <!-- </a> -->
            <br>
            <strong>Shen Zheng</strong>, 
            Gaurav Gupta
            <br>
            <em style="display: block; margin: 10px 0 -5px;"> 
              <strong style="color: red;">WACV 2022</strong> 
            </em>
            <br>
            <a href="https://arxiv.org/pdf/2110.00970.pdf">Paper</a> |
            <!-- <a href="https://openaccess.thecvf.com/content/WACV2022W/RWS/supplemental/Zheng_Semantic-Guided_Zero-Shot_Learning_WACVW_2022_supplemental.pdf">Supp</a> | -->
            <a href="https://github.com/ShenZheng2000/Semantic-Guided-Low-Light-Image-Enhancement">Code</a> |
            <a href="images/SGZ/SGZ_Slides.pdf">Slides</a> |
            <a href="https://www.youtube.com/watch?v=SrNMPqQFncY">Video</a>
            <p>
              <strong>Motivation</strong>: Current low-light image enhancement methods cannot handle uneven illuminations, is computationally inefficient, and fail to preserve the semantic information. 
            </p>
            <p>
              <strong>Solution</strong>: SGZ, a zero-shot low-light image enhancement framework with pixel-wise light deficiency estimation, parameter-free recurrent image enhancement, and unsupervised semantic segmentation.
            </p>
          </td>
        </tr>


        <tr onmouseout="ACML_stop()" onmouseover="ACML_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='acml'>
              <img src="images/AS_IntroVAE/AS_IntroVAE.JPG" width="160"></div>
              <img src='images/AS_IntroVAE/AS_IntroVAE.JPG' width="160">
            </div>
            <script type="text/javascript">
              function ACML_start() {
                document.getElementById('acml').style.opacity = "1";
              }

              function ACML_stop() {
                document.getElementById('acml').style.opacity = "0";
              }
              UDA_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <a href="images/AS_IntroVAE/AS_IntroVAE_Paper.pdf"> -->
              <papertitle>AS-IntroVAE: Adversarial Similarity Distance Makes Robust IntroVAE</papertitle>
            <!-- </a> -->
            <br>
            Changjie Lu, 
            <strong>Shen Zheng</strong>, 
            Zirui Wang,
            Omar Dib,
            Gaurav Gupta
            <br>
            <em style="display: block; margin: 10px 0 -5px;"> 
              <strong style="color: red;">ACML 2022</strong> 
            </em> 
            <br>
            <!-- <a href="">code</a> / -->
            <a href="https://arxiv.org/pdf/2206.13903.pdf">Paper</a> |            
            <a href="https://github.com/ShenZheng2000/SAPNet-for-image-deraining">Code</a> |
            <a href="images/AS_IntroVAE/AS_IntroVAE_Slides.pdf">Slides</a>
            <p>
              <strong>Motivation</strong>: Generative models experience posterior collapse and vanishing gradient due to no effective metric for real-fake image evaluation.
            </p>
            <p>
              <strong>Solution</strong>: Propose Adversarial Similarity Distance Introspective Variational Autoencoder (AS-IntroVAE), which can address the posterior
              collapse and the vanishing gradient problem in image generation in one go. 
            </p>
          </td>
        </tr>


    
          <tr onmouseout="ijcnn_stop()" onmouseover="ijcnn_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='blur'>
                <img src="images/Deblur_YOLO/baby_blur.png" width="160"></div>
                <img src='images/Deblur_YOLO/baby_deblurYolo.png' width="160">
              </div>
              <script type="text/javascript">
                function ijcnn_start() {
                  document.getElementById('blur').style.opacity = "0";
                }

                function ijcnn_stop() {
                  document.getElementById('blur').style.opacity = "1";
                }
                ijcnn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="https://ieeexplore.ieee.org/document/9534352/"> -->
                <papertitle>Deblur-YOLO: Real-Time Object Detection with Efficient Blind Motion Deblurring</papertitle>
              <!-- </a> -->
              <br>
              <strong>Shen Zheng</strong>, 
              Yuxiong Wu,
              Shiyu Jiang,
              Changjie Lu, 
              Gaurav Gupta
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">IJCNN 2021 (Oral Presentation)</strong> 
              </em> 
              <br>
              <a href="https://ieeexplore.ieee.org/document/9534352/">Paper</a> |
              <a href="images/Deblur_YOLO/Deblur_YOLO_Slides.pdf">Slides</a>
              <p>
                <strong>Motivation</strong>: Object detection algorithms exhibit suboptimal performance on blurry scenes.
              </p>
              <p>
                <strong>Solution</strong>: Propose Deblur-YOLO, a generative adversarial network with a dilated feature pyramid generator, double multi-scale discriminators, and a detection discriminator
              to deal with photographs corrupted by motion blur.
              </p>
            </td>
          </tr>

          <tr onmouseout="EES_stop()" onmouseover="EES_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='EES'>
                <img src="images/EES/EES.png" width="160"></div>
                <img src='images/EES/EES.png' width="160">
              </div>
              <script type="text/javascript">
                function EES_start() {
                  document.getElementById('EES').style.opacity = "1";
                }

                function EES_stop() {
                  document.getElementById('EES').style.opacity = "0";
                }
                UDA_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<!-- <a href="images/AS_IntroVAE/AS_IntroVAE_Paper.pdf"> -->
                <papertitle>Efficient Ensemble Sparse Convolutional Neural Networks with Dynamic Batch Size</papertitle>
              <!-- </a> -->
              <br>
              <strong>Shen Zheng</strong>, 
              Liwei Wang,
              Gaurav Gupta
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">CVIP 2020 (Oral Presentation)</strong> 
              </em> 
              <br>
							<!-- <a href="">code</a> / -->
              <a href="https://link.springer.com/chapter/10.1007/978-981-16-1103-2_23">Paper</a> |
              <a href="images/EES/EES_Slides.pdf">Slides</a>
              <p>
                <strong>Motivation</strong>: Existing ConvNets have poor computational complexity and require significant memory consumption.
              </p>  
              <p>
                <strong>Solution</strong>: Introduce an efficient ConvNet with weighted average stacking, Winograd-ReLU-based network pruning, and a electromagnetic-inspired dynamic batch size algorithm.
              </p>
            </td>
          </tr>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Workshop Papers</heading>
              <p>
                <!-- Maybe: summarize main research points -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


      
          <tr onmouseout="rain_stop()" onmouseover="rain_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rainy'>
                <img src="images/SAPNet/rainy.png" width="160"></div>
                <img src='images/SAPNet/derain.png' width="160">
              </div>
              <script type="text/javascript">
                function rain_start() {
                  document.getElementById('rainy').style.opacity = "0";
                }
    
                function rain_stop() {
                  document.getElementById('rainy').style.opacity = "1";
                }
                rain_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Zheng_SAPNet_Segmentation-Aware_Progressive_Network_for_Perceptual_Contrastive_Deraining_WACVW_2022_paper.pdf"> -->
                <papertitle>SAPNet: Segmentation-Aware Progressive Network for Perceptual Contrastive Deraining</papertitle>
              <!-- </a> -->
              <br>
              <strong>Shen Zheng</strong>, 
              Changjie Lu, 
              Yuxiong Wu, 
              Gaurav Gupta
              <br>
              <em style="display: block; margin: 10px 0 -5px;"> 
                <strong style="color: red;">WACVW 2022</strong> 
              </em>
              <br>
              <a href="https://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Zheng_SAPNet_Segmentation-Aware_Progressive_Network_for_Perceptual_Contrastive_Deraining_WACVW_2022_paper.pdf">Paper</a> |
              <a href="https://openaccess.thecvf.com/content/WACV2022W/VAQ/supplemental/Zheng_SAPNet_Segmentation-Aware_Progressive_WACVW_2022_supplemental.pdf">Supp</a> |
              <a href="https://github.com/ShenZheng2000/SAPNet-for-image-deraining">Code</a> |
              <a href="images/SAPNet/SAPNet_Slides.pdf">Slides</a> |
              <a href="https://www.youtube.com/watch?v=tJSsICHpsfs">Video</a>
              <p>
                <strong>Motivation</strong>: Former deraining approaches often eliminate essential background details along with the rain, hindering tasks such as detection and segmentation. 
              </p>
                <p>
                  <strong>Solution</strong>: SAPNet, an image-deraining network that integrates low-level image-deraining and high-level background segmentation using progressive dilated unit, perceptual contrastive loss, and unsupervised background segmentation.
              </p>
            </td>
          </tr>
    
    
    
    
    
    
            <!-- Maybe: Use images, not figures -->
              <tr onmouseout="UDA_stop()" onmouseover="UDA_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='UDA_VAE++'>
                    <img src="images/UDA_VAE++/UDA_VAE++.png" width="160"></div>
                    <img src='images/UDA_VAE++/UDA_VAE++.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function UDA_start() {
                      document.getElementById('mine').style.opacity = "1";
                    }
    
                    function UDA_stop() {
                      document.getElementById('mine').style.opacity = "0";
                    }
                    UDA_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="https://arxiv.org/pdf/2204.09334.pdf"> -->
                    <papertitle>Unsupervised Domain Adaptation for Cardiac Segmentation: Towards Structure
                      Mutual Information Maximization</papertitle>
                  <!-- </a> -->
                  <br>
                  Changjie Lu, 
                  <strong>Shen Zheng</strong>, 
                  Gaurav Gupta
                  <br>
                  <em style="display: block; margin: 10px 0 -5px;"> 
                    <strong style="color: red;">CVPRW 2022</strong> 
                  </em> 
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Lu_Unsupervised_Domain_Adaptation_for_Cardiac_Segmentation_Towards_Structure_Mutual_Information_CVPRW_2022_paper.pdf">Paper</a> |
                  <a href="https://openaccess.thecvf.com/content/CVPR2022W/Precognition/supplemental/Lu_Unsupervised_Domain_Adaptation_CVPRW_2022_supplemental.pdf">Supp</a> |
                  <a href="https://github.com/LOUEY233/Toward-Mutual-Information">Code</a> |
                  <a href="images/UDA_VAE++/UDA_VAE++.pdf">Slides</a> 
    
                  <p>
                    <strong>Motivation</strong>: Previous unsupervised domain adaptation methods for medical imaging falter across varied imaging modalities due to substantial domain differences.
                  </p>
                  <p>
                    <strong>Solution</strong>: UDA-VAE++, an unsupervised domain adaptation framework that leverages mutual information maximization and sequential reparameterization for cardiac segmentation.
                  </p>
                </td>
              </tr>
    




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Internships</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Momenta/Momenta.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong>Computer Vision Engineer, Perception </strong> at <a href="https://www.momenta.cn/">Momenta</a>

              <p>
                Director: <a href="https://scholar.google.com.sg/citations?user=Wo5Cem4AAAAJ&hl=en">Dr. Wangjiang Zhu</a>
                 
              </p>

              <p>
              Responsible for long-tailed data augmentation, training data auto-labeling and cleaning, and model evaluation for traffic light detection algorithms.
            </p>

            <p>
              Implemented CycleGAN to conduct unsupervised data augmentation, converting traffic light bulbs from left arrow to round & leftUturn arrow.
            </p>

            <p>
              Constructed a traffic light auto-label model using quantized VoVNet-57, filtering 14,618 incorrect annotations from 1,160,513 labeled frames.
            </p>

            <p>
              Increased the classification accuracy for leftUturn traffic light from 78.41% to 87.27%, and the mean average precision from 93.01% to 94.80%.
            </p>
            </td>
          </tr>

<!--             
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/UND/UND.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong> Research Assistant </strong> at <a href="https://www.nd.edu/">University of Notre Dame</a>

              <p>
                Supervisor: <a href="https://sites.nd.edu/chaoli-wang/">Dr. Chaoli Wang</a>
                 
              </p>

              <p>
              Developed a fully convolutional neural network with Siren activation function to render isosurfaces with image resolution, viewpoints and isovalue.
            </p>

              <p>
              Leveraged Greene's bisection method and Jacobian matrix's eigenvalue for critical point detection and classification in the simulated 3D isosurface.
            </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/China_Life/China_Life.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong>Sales Analyst Intern </strong> at <a href="https://www.chinalife.com.hk/zh-cn">China Life insurance Company</a>

              <p>
                Applied K-means clustering model to classify text data statewide as three significant categories to eliminate the risk from over 20,000 unannounced expired insurance from 7 cities.
              </p>

              <p>
                Employed t-test and Adjusted R Squared to help Sales Manager and General Manager deciding the bonus percentage for consecutive monthly sales as 6.00%.
              </p>

            </td>
          </tr> -->





        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Services/CVF.jpg" style="width: 185px;"></td>
            <td width="75%" valign="center">

              <strong>Technical Program Committee</strong>: 
              <br>
              <a href="https://2024.ieeewcci.org/">WCCI 2024</a>
              <br>
              <br>

              <strong>Conference Reviewers</strong>: 
              <br>
              <a href="https://sites.google.com/view/cvip-2021/home">CVIP 2021</a>, 
              <a href="https://vnit.ac.in/cvip2022/">CVIP 2022</a>, 
              <a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a>, 
              <a href="https://2023.ijcnn.org/">IJCNN 2023</a>, 
              <a href="https://wacv2023.thecvf.com/home">WACV 2023</a>,
              <a href="https://wacv2024.thecvf.com/home">WACV 2024</a>
              <br>
              <br>

              <strong>Journal Reviewers</strong>:
              <br>
              <a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems">TNNLS</a>,
              <a href="https://www.springer.com/journal/11263">IJCV</a>
              
              <br>
              <br>

              <strong>Session Chair</strong>:
              <br>
              <a href="https://dblp.org/db/conf/ijcnn/ijcnn2021.html">IJCNN 2021</a>
              <br>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/WKU/WKU.png" style="width: 185px;"></td>
            <td width="75%" valign="center">
              <strong>Co-Instructor</strong> at <a href="https://www.wku.edu.cn/en/">Wenzhou-Kean University</a>

              <br>
              <br>
              <strong>Course</strong>: MATH 3291/3292 (Computer Vision)
              <br>
              <br>
              
              <a href="https://drive.google.com/drive/folders/1iYJHC2E_v-YYGZXK6BKJ543_o--p-RR4">Slide</a> | 
              <a href="https://drive.google.com/drive/folders/1ZiHHAlPxVj727sq7MyK9Lmpcusp3bip3">Recordings</a>
              
              <!-- <br>
              Collected state-of-the-art paper lists for paper reading, paper discussion, and literature review.  -->

              <!-- <br>
              I take the responsibility of 
              <a href="https://github.com/WKUAILAB/AI_Tutorial/tree/main/CV">Computer Vision</a>. -->
            </td>
          </tr>



      

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Services/Fudan.png" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong> Invited Speaker </strong> at <a href="https://www.fudan.edu.cn/en/">Fudan University</a>
              <br>
              <br>
              <strong>Topic</strong>: <a href="images/Services/Slides_Fudan.pdf">Image Processing with Machine Learning</a>
              <br>
            </td>
          </tr>


  
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Services/AI_Lab.jpg" style="width: 185px;"></td>
            <td width="75%" valign="center">
              <strong>Co-founder</strong> of <a href="https://github.com/WKUAILAB">WKU AI-LAB</a>

              <br>
              <br>
              Offered AI Tutorials in Computer Vision and Natural Language Processing for undegraduate students. 
              <br>
              
              <!-- <br>
              Collected state-of-the-art paper lists for paper reading, paper discussion, and literature review.  -->

              <!-- <br>
              I take the responsibility of 
              <a href="https://github.com/WKUAILAB/AI_Tutorial/tree/main/CV">Computer Vision</a>. -->
            </td>
          </tr>

                      
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Services/Leetcode.jpg" style="width: 185px;">
            </td>
            <td width="75%" valign="center">
              <strong> Content Creator</strong>: 
              <br>
              Made 100+ <a href="https://leetcode.com/">YouTube</a> video solutions for <a href="https://leetcode.com/">Leetcode</a> algorithms questions.
            </td>
          </tr>




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Skills</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Skills/Skills.jpeg" style="width: 185px;"></td>
            <td width="75%" valign="center">
              <!-- <a href="https://github.com/WKUAILAB">WKU AI-LAB</a> -->

              <strong>Programming Languages</strong>: 
              <br>
              Python, R, Java, C++, Matlab, HTML, Mathematica, Shell, LaTeX, Markdown
              <br>
              <br>

              <strong>Frameworks & Platforms</strong>: 
              <br>
              Pytorch, TensorFlow, Keras, Ubuntu, Docker, Git, ONNX, CUDA
              <br>
              <br>


              <strong>Libraries</strong>: 
              <br>
              Scikit-Learn, SciPy, NumPy, OpenCV, Matplotlib, Pandas
              <br>
              <br>
            </td>
          </tr>



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Fun Facts</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Fun_Facts/Fun_Facts.jpg" style="width: 185px;"></td>
              <td width="75%" valign="center">
                <!-- <a href="https://github.com/WKUAILAB">WKU AI-LAB</a> -->
  
                <strong>Languages</strong>: 
                <br>
                Chinese, English, Spanish, Russian, Arabic
                <br>
                <br>

                <strong>Sports</strong>: 
                <br>
                Basketball, Table Tennis, Swimming, Cycling, Hiking, Weightlifting
                <br>
                <br>

                <strong>Games</strong>: 
                <br>
                DOTA2, AOE2, Warcraft III
                <br>
                <br>

                <strong>Beliefs</strong>: 
                <br>
                <a href="https://yunuoch.github.io/">‰∫àËØ∫‰∏âËßÇ</a> &
                <a href="https://www.linkedin.com/in/haoran-qian/">Êµ©ÁÑ∂ÈÅìË∑Ø</a> &
                <a href="https://www.researchgate.net/profile/Liwei-Wang-15">Á´ãÂ®ÅÊÄùÊÉ≥</a> &
                <a href="https://www.linkedin.com/in/hang-xie/">Ë∞¢Ëà™Á≤æÁ•û</a> &
                <a href="http://relic.wang/">Ê∞∏ÂØåÊñπÊ≥ï</a> &
                <a href="https://www.linkedin.com/in/yuanzhe-liu-0a8a9a1b2/">ÂàòËøúÂì≤Â≠¶</a> 
                <br>

  

                <br>
              </td>
            </tr>
  

<!-- 
          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Visitors</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.revolvermaps.com/livestats/5xcf5hcpcuz/"><img src="//rf.revolvermaps.com/h/m/a/0/ff0000/128/0/5xcf5hcpcuz.png" width="256" height="128" alt="Map" style="border:0;"></a>      </td>
            </td>
          </tr> -->


    </tr>
  </table>
</body>

</html>
